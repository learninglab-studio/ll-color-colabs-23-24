{"cells":[{"cell_type":"markdown","metadata":{"id":"xVesm5oiZS2g"},"source":["## installation and set up"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19067,"status":"ok","timestamp":1713806198027,"user":{"displayName":"LL Yellow","userId":"09695474355769494704"},"user_tz":240},"id":"HFvXoXCubS4s","outputId":"d01cfe9e-2412-4ae1-ca60-e3c254ce6942"},"outputs":[],"source":["!pip install openai"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26862,"status":"ok","timestamp":1713806224884,"user":{"displayName":"LL Yellow","userId":"09695474355769494704"},"user_tz":240},"id":"cSlGNND5ZHMk","outputId":"d9b1d845-081e-49c4-a7a8-432595514478"},"outputs":[],"source":["# setup\n","\n","from google.colab import userdata\n","OPENAI_API_KEY=userdata.get('GROUP1_OPENAI_API_KEY')\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","from openai import OpenAI\n","\n","client = OpenAI(\n","    api_key = OPENAI_API_KEY\n",")"]},{"cell_type":"markdown","metadata":{"id":"2i8gCNeYqvto"},"source":["### define assistants\n","\n","for this tutorial we are going to assume that you have created your assistants in the openai assistant playground (you can also test them out there). Once you've done that, add them to the list below and choose the one(s) you'd like to use."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DC0bg7_2bJ4a"},"outputs":[],"source":["# add your assistants here (or just hard code a single assistant as the one you want to use)\n","from google.colab import userdata\n","\n","# Retrieve assistant IDs from user data\n","neanderthal_id = userdata.get('neanderthal_assistant_id')\n","proto_indo_european_id = userdata.get('proto_indo_european_assistant_id')\n","rising_star_id = userdata.get('rising_star_assistant_id')\n","sanxingdui_id = userdata.get('sanxingdui_assistant_id')\n","\n","group1_assistants = {\n","    \"neanderthal\": neanderthal_id,\n","    \"proto_indo_european\": proto_indo_european_id,\n","    \"rising_star\": rising_star_id,\n","    \"sanxingdui\": sanxingdui_id\n","}\n","\n","assistant_id = group1_assistants[\"sanxingdui\"]"]},{"cell_type":"markdown","metadata":{"id":"NGV83z0PPMgu"},"source":["## update assistant with functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3AVnDlVGPRnI"},"outputs":[],"source":["# function schema sample\n","my_function = {\n","  \"type\": \"function\",\n","  \"function\": {\n","    \"name\": \"get_themes\",\n","    \"description\": \"Gets an array of AT LEAST 3 themes from the list of themes in response to any question asking for a story, folk tale, myth, or related narrative. These 3 themes should be the best for the type of story that the user has asked for.\",\n","     \"parameters\": {\n","      \"type\": \"object\",\n","      \"properties\": {\n","        \"themes\": {\n","          \"type\": \"array\",\n","          \"description\": \"List containing at between 3 and 7 themes selected from the specified array of themes\",\n","          \"minItems\": 3,\n","          \"items\": {\n","            \"type\": \"string\",\n","            \"enum\": [\n","              \"Adventure\",\n","              \"Courage\",\n","              \"Friendship\",\n","              \"Betrayal\",\n","              \"Love\",\n","              \"Mystery\",\n","              \"Quest\",\n","              \"Revenge\",\n","              \"Sacrifice\",\n","              \"Survival\",\n","              \"Tragedy\",\n","              \"Victory\"\n","            ]\n","          }\n","        }\n","      },\n","      \"required\": [\"themes\"]\n","    }\n","  }\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":321,"status":"ok","timestamp":1712938226037,"user":{"displayName":"LL Yellow","userId":"09695474355769494704"},"user_tz":240},"id":"Ky_NDzdGA8ja","outputId":"6e914c68-0c4e-4dc2-d9a7-13fcf5ba9a44"},"outputs":[],"source":["# update assistant with functions\n","\n","import datetime\n","from openai import OpenAI\n","\n","timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n","# from previous step in this colab\n","new_assisant_name = f\"demo-bot-w-functions-{timestamp}\"\n","\n","\n","my_updated_assistant = client.beta.assistants.update(\n","  assistant_id,\n","  name=new_assisant_name,\n","  tools=[{\"type\": \"retrieval\"}, my_function],\n","  model=\"gpt-4-turbo-preview\"\n",")\n","\n","print(my_updated_assistant)"]},{"cell_type":"markdown","metadata":{"id":"Dkw21kSXbOwl"},"source":["## thread cycle"]},{"cell_type":"markdown","metadata":{"id":"nuSk4uSqo3QY"},"source":["### create"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":646,"status":"ok","timestamp":1712939836726,"user":{"displayName":"LL Yellow","userId":"09695474355769494704"},"user_tz":240},"id":"-HH3yla9bQ0t","outputId":"be734d9e-67f8-4eb1-b0d6-7711f1f2f61f"},"outputs":[],"source":["thread = client.beta.threads.create()\n","\n","message = client.beta.threads.messages.create(\n","    thread_id=thread.id,\n","    role=\"user\",\n","    content=\"can you create a myth about solar eclipses, where they came from, etc? Please do so by referencing any works you can retrieve (and tell me which works you referenced)\"\n",")\n","\n","print(thread)\n"]},{"cell_type":"markdown","metadata":{"id":"PjU4_Nehm_BO"},"source":["### create the \"run\" which includes a message and the final prompt (PROMPT A)"]},{"cell_type":"markdown","metadata":{"id":"D4pm13FFnLDO"},"source":["### run the thread"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhsgcNvmCRzt"},"outputs":[],"source":["run = client.beta.threads.runs.create(\n","    thread_id=thread.id,\n","    assistant_id=assistant_id\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QUFLRgwflCzI"},"outputs":[],"source":["import json\n","\n","print(json.dumps(run.dict(), indent=4))"]},{"cell_type":"markdown","metadata":{"id":"7rbvjflhn_V7"},"source":["### check on the thread\n","\n","and wait for completion (could take a minute or so)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":336,"status":"ok","timestamp":1712939878604,"user":{"displayName":"LL Yellow","userId":"09695474355769494704"},"user_tz":240},"id":"dinJS5XfZhpf","outputId":"41d548b4-5c29-4230-cea2-63595ae3a550"},"outputs":[],"source":["import time\n","import json\n","\n","def wait_for_completion(client, thread_id, run_id):\n","    while True:\n","        run_info = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\n","        print(f\"Current run status: {run_info.status}\")\n","\n","        if run_info.status == \"completed\":\n","            print(\"Run is complete.\")\n","            break\n","        elif run_info.status == \"requires_action\":\n","            print(f\"Run requires action.\")\n","            break\n","\n","        else:\n","            # Wait for a short period before checking again if the run is in any other state.\n","            print(f\"Not done yet, status is {run_info.status}\")\n","            time.sleep(5)  # Adjust sleep time as needed.\n","\n","    return run_info\n","\n","run_info = wait_for_completion(client, thread.id, run.id)\n","\n","print(json.dumps(run_info.dict(), indent=4))"]},{"cell_type":"markdown","metadata":{"id":"rwlOJGnGnOq-"},"source":["### retrieve the output and handle tool calls\n","\n","\n","plus new logic that sorts through ALL messages and only returns the last-produced JSON string."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1712939885359,"user":{"displayName":"LL Yellow","userId":"09695474355769494704"},"user_tz":240},"id":"Ty21z4UUF0l6","outputId":"8e45df4a-dbf8-4fbd-d5e4-2e86a90aaccf"},"outputs":[],"source":["arguments_to_handle = run_info.required_action.submit_tool_outputs.tool_calls[0].function.arguments\n","tool_call_id = run_info.required_action.submit_tool_outputs.tool_calls[0].id\n","\n","print(arguments_to_handle)\n","print(tool_call_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21397,"status":"ok","timestamp":1712939912155,"user":{"displayName":"LL Yellow","userId":"09695474355769494704"},"user_tz":240},"id":"ne2dQF0-GRml","outputId":"971d0ac3-248a-4b37-e799-264f40defd0d"},"outputs":[],"source":["# ask a different assistant to do something with this\n","\n","import textwrap\n","\n","def text_to_text(prompt):\n","    system_prompt = \"You are a helpful assistant with a wealth of knowledge about folklore and myth from a variety of cultures across the globe.\"\n","    user_prompt = prompt\n","    response = client.chat.completions.create(\n","        model=\"gpt-4-turbo-preview\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": system_prompt},\n","            {\"role\": \"user\", \"content\": user_prompt}\n","        ]\n","    )\n","    wrapped_text = textwrap.fill(response.choices[0].message.content, width=500)\n","    print(wrapped_text)\n","    return response.choices[0].message.content\n","\n","tool_call_processed_content = text_to_text(f\"can you help explain what Joseph Campbell would say about these themes? {json.dumps(arguments_to_handle)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q78NceEtGUyg"},"outputs":[],"source":["run = client.beta.threads.runs.submit_tool_outputs(\n","  thread_id=thread.id,\n","  run_id=run.id,\n","  tool_outputs=[\n","      {\n","        \"tool_call_id\": tool_call_id,\n","        \"output\": tool_call_processed_content\n","      }\n","    ]\n",")"]},{"cell_type":"markdown","metadata":{"id":"NFRQ_dayRZZ9"},"source":["### retrieve again until done"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10853,"status":"ok","timestamp":1712939949938,"user":{"displayName":"LL Yellow","userId":"09695474355769494704"},"user_tz":240},"id":"79Z0FbG3LOK_","outputId":"1ffa48fa-2f3e-4090-d54a-dbd5606113d5"},"outputs":[],"source":["import time\n","import json\n","\n","def wait_for_completion(client, thread_id, run_id):\n","    while True:\n","        run_info = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\n","        print(f\"Current run status: {run_info.status}\")\n","\n","        if run_info.status == \"completed\":\n","            print(\"Run is complete.\")\n","            break\n","        elif run_info.status == \"requires_action\":\n","            print(f\"Run requires action.\")\n","            break\n","\n","        else:\n","            # Wait for a short period before checking again if the run is in any other state.\n","            print(f\"Not done yet, status is {run_info.status}\")\n","            time.sleep(5)  # Adjust sleep time as needed.\n","\n","    return run_info\n","\n","run_info = wait_for_completion(client, thread.id, run.id)\n","\n","print(json.dumps(run_info.dict(), indent=4))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":524,"status":"ok","timestamp":1712939956532,"user":{"displayName":"LL Yellow","userId":"09695474355769494704"},"user_tz":240},"id":"ZhG7zmBsj1qf","outputId":"c5626769-dcc6-4165-cd24-9edd6148cf85"},"outputs":[],"source":["import json\n","import textwrap\n","\n","messages = client.beta.threads.messages.list(thread_id=thread.id)\n","\n","# FOR ALL MESSAGES UNCOMMENT THIS\n","\n","# for message in messages.data:\n","#     message_info = client.beta.threads.messages.retrieve(\n","#         thread_id=thread.id,\n","#         message_id=message.id\n","#     )\n","#     print(json.dumps(message_info.dict(), indent=4))\n","#     message_text = message_info.content[0].text.value\n","#     print(textwrap.fill(message_text, width=50))\n","\n","# JUST MOST RECENT MESSAGE\n","\n","message_info = client.beta.threads.messages.retrieve(\n","    thread_id=thread.id,\n","    message_id=messages.data[0].id\n",")\n","\n","print(json.dumps(message_info.dict(), indent=4))\n","message_text = message_info.content[0].text.value\n","print(textwrap.fill(message_text, width=50))"]},{"cell_type":"markdown","metadata":{"id":"VFdMp8KMRfxf"},"source":["### optional = handle citations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVmNGBTJFcx-"},"outputs":[],"source":["def process_citations(client, message_content):\n","    # Copy the original message content for this run\n","    modified_content = message_content.value[:]\n","    annotations = message_content.annotations if hasattr(message_content, 'annotations') else []\n","    citations = []\n","\n","    for index, annotation in enumerate(annotations):\n","        # Replace the text with a footnote marker\n","        modified_content = modified_content.replace(annotation.text, f' [{index}]')\n","\n","        # Process file citations and paths\n","        if (file_citation := getattr(annotation, 'file_citation', None)):\n","            cited_file = client.files.retrieve(file_citation.file_id)\n","            citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n","        elif (file_path := getattr(annotation, 'file_path', None)):\n","            cited_file = client.files.retrieve(file_path.file_id)\n","            citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n","            # Placeholder for actual file download link or method\n","\n","    # Append gathered citations, ensuring they start on a new line\n","    if citations:\n","        modified_content += '\\n\\n' + '\\n'.join(citations)  # Ensure two newlines before starting the citations\n","\n","    # Wrap the modified content, including citations, with 50 character line width\n","    wrapped_content = textwrap.fill(modified_content, width=50)\n","\n","    return wrapped_content\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":587,"status":"ok","timestamp":1712940039744,"user":{"displayName":"LL Yellow","userId":"09695474355769494704"},"user_tz":240},"id":"V93txgyvLrRr","outputId":"ba13d212-57ac-4d63-c58a-f4479b621f86"},"outputs":[],"source":["original_message_content=message_info.content[0].text\n","\n","# Use the function to process and wrap your content, adjust function parameters as needed\n","wrapped_and_processed_content = process_citations(client, original_message_content)\n","\n","# Print the wrapped and processed content with correct newline before citations\n","print(wrapped_and_processed_content)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1-oOYCJOjWdJGtmHfzn211qO-Y4QnfGNQ","timestamp":1712774876826}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
