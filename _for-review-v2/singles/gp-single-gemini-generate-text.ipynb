{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25665,"status":"ok","timestamp":1712589386391,"user":{"displayName":"Marlon Kuzmick","userId":"05330468330403584675"},"user_tz":240},"id":"sgVP_UKjb8Jw","outputId":"816d6f66-10ae-448d-a405-3d7e0dd33319"},"outputs":[],"source":["!pip install -q -U google-generativeai"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":921},"executionInfo":{"elapsed":9415,"status":"ok","timestamp":1709223859303,"user":{"displayName":"Gonzalo Pelenur","userId":"06533601451551970807"},"user_tz":300},"id":"dHabNHThcOZE","outputId":"9170f38c-4c96-4330-8d59-e6ab362961ff"},"outputs":[],"source":["# @title Generate Text\n","prompt = \"tell me about google gemini\"\n","import pathlib\n","import textwrap\n","\n","import google.generativeai as genai\n","\n","from IPython.display import display\n","from IPython.display import Markdown\n","\n","from google.colab import userdata\n","\n","def to_markdown(text):\n","  text = text.replace('â€¢', '  *')\n","  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n","\n","\n","\n","GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n","\n","genai.configure(api_key=GOOGLE_API_KEY)\n","model = genai.GenerativeModel('gemini-pro')\n","response = model.generate_content(prompt)\n","\n","print(\"here is some feedback on your prompt\")\n","print(response.prompt_feedback)\n","\n","to_markdown(response.text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":868},"executionInfo":{"elapsed":22590,"status":"ok","timestamp":1709224006935,"user":{"displayName":"Gonzalo Pelenur","userId":"06533601451551970807"},"user_tz":300},"id":"WxhevUMPrXGK","outputId":"11a1d6a1-7b29-4ca4-98b3-9614e0bb7a36"},"outputs":[],"source":["# @title Stream Response\n","from time import sleep\n","prompt = \"tell me about google gemini\"\n","\n","response = model.generate_content(prompt, stream=True)\n","for chunk in response:\n","  print(chunk.text)\n","  print(\"_\"*50)\n","  sleep(0.3)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
